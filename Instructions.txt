Project Title:Â AIRA- AI-Powered Research Agent with MCP, RAG, Vector DB, LangChain, and Gemini API

Abstract:

AIRA is an intelligent research assistant designed to retrieve, summarize, and answer questions using scientific literature. It combines local and online paper sources, multi-source context routing (via MCP), a vector database for document embedding and retrieval, and Gemini LLM for high-quality summarization and Q&A. The system mimics a chatbot that can intelligently answer complex research questions with context from both APIs (e.g., arXiv) and the userâ€™s own documents. It includes memory, citation generation, and modular components for scalability and learning.

End-to-End Implementation Instructions:

ğŸ” 1. Environment Setup

Create project folder:

mkdir ai-research-agent && cd ai-research-agent
git init

Create virtual environment and activate:

python -m venv .venv
.venv\Scripts\activate  # On Windows
source .venv/bin/activate  # On Mac/Linux

Install base dependencies:

pip install flask langchain chromadb python-dotenv streamlit
pip freeze > requirements.txt

ğŸ“‚ 2. Folder Structure

ai-research-agent/
â”œâ”€â”€ mcp_server.py
â”œâ”€â”€ context_router.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ gemini_llm.py
â”‚   â””â”€â”€ memory.py
â”œâ”€â”€ context_sources/
â”‚   â”œâ”€â”€ arxiv_api.py
â”‚   â”œâ”€â”€ user_local_files.py
â”‚   â””â”€â”€ github_docs.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ mcp_schema.py
â”œâ”€â”€ vector_store/
â”œâ”€â”€ data/
â”œâ”€â”€ logs/
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â””â”€â”€ graphs/
    â””â”€â”€ langgraph_workflow.py

ğŸ› ï¸ 3. MCP Server (mcp_server.py)

Set up Flask route /query

Parse JSON payload: query, context_sources

Call fetch_all_context(query, sources) from context_router.py

Return dummy context (initially) or later, pass to RAG pipeline

ğŸ”„ 4. Context Router (context_router.py)

Define a dictionary of source-specific fetchers

Implement fetch_all_context(query, sources)

Dynamically call each fetcher and return combined context

ğŸ§  5. Source Fetchers (context_sources/*.py)

arxiv_api.py: Connect to arXiv API or return dummy papers

user_local_files.py: Read .pdf or .txt files from data/

github_docs.py: Clone repo or read markdown files

ğŸ” 6. Embedding + Vector DB (modules/rag_pipeline.py)

Use Sentence Transformers to embed paper chunks

Store and retrieve with ChromaDB

Implement similarity search for user queries

ğŸ’¡ 7. Gemini Integration (modules/gemini_llm.py)

Set up google.generativeai client with API key

Create prompt templates for:

Summarization

Direct Q&A

Citation response

ğŸ“… 8. Memory (modules/memory.py)

Use LangChain ConversationBufferMemory

Store memory per session/user

ğŸ” 9. Secure API Keys (.env)

GEMINI_API_KEY=your_api_key_here

Use dotenv.load_dotenv() in gemini_llm.py to access the key.

ğŸ” 10. Test the API Endpoint

Use PowerShell:

Invoke-RestMethod -Uri http://127.0.0.1:5000/query -Method POST -ContentType "application/json" -Body '{"query": "What is federated learning?", "context_sources": ["arxiv_api", "user_local_files"]}'

ğŸ“˜ 11. LangGraph Workflow (graphs/langgraph_workflow.py)

Install: pip install langgraph

Define workflow graph (nodes = fetch, rerank, summarize)

Use LangChain tools as nodes; define pathing logic for flow

Run graph with input query and context

ğŸ“ˆ 12. LangSmith Monitoring

Install: pip install langsmith

Create account and get API key from https://smith.langchain.com

Track session, node performance, and hallucinations

Add logging inside LangChain chains with LangSmith handlers

ğŸ’¬ 13. Streamlit Chatbot UI (ui/streamlit_app.py)

Install: pip install streamlit

Use st.chat_input() for user queries

Display streamed Gemini response

Save past messages to show chat memory

Add dropdown to select source types

Run with: streamlit run ui/streamlit_app.py